{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.14","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":89760,"databundleVersionId":10486094,"sourceType":"competition"}],"dockerImageVersionId":30805,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import os\nimport torch\nimport torch.nn as nn\nimport torch.optim as optim\nfrom torch.utils.data import DataLoader, Dataset\nimport torch.utils.checkpoint as checkpoint\nfrom torchvision import models, transforms\nfrom torchvision.models import vgg19\nfrom torchvision import transforms\nfrom PIL import Image\nfrom tqdm import tqdm","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-15T19:54:59.963153Z","iopub.execute_input":"2024-12-15T19:54:59.964189Z","iopub.status.idle":"2024-12-15T19:55:04.365971Z","shell.execute_reply.started":"2024-12-15T19:54:59.964142Z","shell.execute_reply":"2024-12-15T19:55:04.365013Z"}},"outputs":[],"execution_count":1},{"cell_type":"code","source":"# del noisy, gt, fake_images, fake_pred\ntorch.cuda.empty_cache()\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-15T19:55:04.367741Z","iopub.execute_input":"2024-12-15T19:55:04.368242Z","iopub.status.idle":"2024-12-15T19:55:04.372482Z","shell.execute_reply.started":"2024-12-15T19:55:04.368206Z","shell.execute_reply":"2024-12-15T19:55:04.371529Z"}},"outputs":[],"execution_count":2},{"cell_type":"code","source":"# Dataset Class\nclass LowLightDataset(Dataset):\n    def __init__(self, noisy_dir, gt_dir=None, transform_noisy=None, transform_gt=None):\n        self.noisy_dir = noisy_dir\n        self.gt_dir = gt_dir\n        self.noisy_files = sorted(os.listdir(noisy_dir))\n        self.gt_files = sorted(os.listdir(gt_dir)) if gt_dir else None\n        self.transform_noisy = transform_noisy\n        self.transform_gt = transform_gt\n\n    def __len__(self):\n        return len(self.noisy_files)\n\n    def __getitem__(self, idx):\n        # Load noisy image\n        noisy_path = os.path.join(self.noisy_dir, self.noisy_files[idx])\n        noisy_image = Image.open(noisy_path).convert('RGB')\n        if self.transform_noisy:\n            noisy_image = self.transform_noisy(noisy_image)\n\n        # Load ground truth image if available\n        if self.gt_dir:\n            gt_path = os.path.join(self.gt_dir, self.gt_files[idx])\n            gt_image = Image.open(gt_path).convert('RGB')\n            if self.transform_gt:\n                gt_image = self.transform_gt(gt_image)\n            return noisy_image, gt_image\n\n        return noisy_image\n\n# Transformations\ntransform_noisy = transforms.Compose([\n    transforms.ToTensor(),\n    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])  # ResNet normalization\n])\n\ntransform_gt = transforms.Compose([\n    transforms.ToTensor(),\n    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n])\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-15T19:55:04.373668Z","iopub.execute_input":"2024-12-15T19:55:04.373989Z","iopub.status.idle":"2024-12-15T19:55:04.391332Z","shell.execute_reply.started":"2024-12-15T19:55:04.373958Z","shell.execute_reply":"2024-12-15T19:55:04.390368Z"}},"outputs":[],"execution_count":3},{"cell_type":"code","source":"# Dataset Paths\ntrain_noisy_dir = \"/kaggle/input/enhance-the-dark-world/archive/train/train\"\ntrain_gt_dir = \"/kaggle/input/enhance-the-dark-world/archive/train/gt\"\nval_noisy_dir = \"/kaggle/input/enhance-the-dark-world/archive/val/val\"\nval_gt_dir = \"/kaggle/input/enhance-the-dark-world/archive/val/gt\"\n\ntest_noisy_dir = \"/kaggle/input/enhance-the-dark-world/archive/test\"\n\n# Datasets and DataLoaders\ntrain_dataset = LowLightDataset(train_noisy_dir, train_gt_dir, transform_noisy, transform_gt)\nval_dataset = LowLightDataset(val_noisy_dir, val_gt_dir, transform_noisy, transform_gt)\ntest_dataset = LowLightDataset(test_noisy_dir, transform_noisy=transform_noisy)\n\n# train_loader = DataLoader(train_dataset, batch_size=8, shuffle=True, num_workers=4)\n# val_loader = DataLoader(val_dataset, batch_size=8, shuffle=False, num_workers=4)\n# test_loader = DataLoader(test_dataset, batch_size=8, shuffle=False, num_workers=4)\n\ntrain_loader = DataLoader(train_dataset, batch_size=8, shuffle=True, num_workers=4)\nval_loader = DataLoader(val_dataset, batch_size=8, shuffle=False, num_workers=4)\ntest_loader = DataLoader(test_dataset, batch_size=8, shuffle=False, num_workers=4)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-15T19:55:04.393717Z","iopub.execute_input":"2024-12-15T19:55:04.394066Z","iopub.status.idle":"2024-12-15T19:55:04.440420Z","shell.execute_reply.started":"2024-12-15T19:55:04.394033Z","shell.execute_reply":"2024-12-15T19:55:04.439343Z"}},"outputs":[],"execution_count":4},{"cell_type":"code","source":"# Test the train loader\nfor noisy_img, gt_img in train_loader:\n    print(f\"Noisy Image Shape: {noisy_img.shape}, GT Image Shape: {gt_img.shape}\")\n    break\n\n# Test the val loader\nfor noisy_img, gt_img in val_loader:\n    print(f\"Noisy Image Shape: {noisy_img.shape}, GT Image Shape: {gt_img.shape}\")\n    break\n\n# Test the test loader\nfor noisy_img in test_loader:\n    print(f\"Noisy Image Shape: {noisy_img.shape}\")\n    break\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-15T19:55:04.441596Z","iopub.execute_input":"2024-12-15T19:55:04.441922Z","iopub.status.idle":"2024-12-15T19:55:07.415103Z","shell.execute_reply.started":"2024-12-15T19:55:04.441891Z","shell.execute_reply":"2024-12-15T19:55:07.413751Z"}},"outputs":[{"name":"stdout","text":"Noisy Image Shape: torch.Size([8, 3, 160, 256]), GT Image Shape: torch.Size([8, 3, 640, 1024])\nNoisy Image Shape: torch.Size([8, 3, 160, 256]), GT Image Shape: torch.Size([8, 3, 640, 1024])\nNoisy Image Shape: torch.Size([8, 3, 160, 256])\n","output_type":"stream"}],"execution_count":5},{"cell_type":"code","source":"# Model Architecture","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-15T19:55:07.416609Z","iopub.execute_input":"2024-12-15T19:55:07.417024Z","iopub.status.idle":"2024-12-15T19:55:07.422369Z","shell.execute_reply.started":"2024-12-15T19:55:07.416986Z","shell.execute_reply":"2024-12-15T19:55:07.421290Z"}},"outputs":[],"execution_count":6},{"cell_type":"code","source":"# import torch\n# import torch.nn as nn\n# import torch.utils.checkpoint as checkpoint\n\n# # Define Generator Network\n# class Generator(nn.Module):\n#     def __init__(self):\n#         super(Generator, self).__init__()\n#         self.conv1 = nn.Conv2d(3, 16, kernel_size=9, stride=1, padding=4)  # Reduced channels\n#         self.prelu = nn.PReLU()\n#         self.res_blocks = nn.ModuleList([ResidualBlock(16) for _ in range(2)])  # Fewer residual blocks\n#         self.conv2 = nn.Conv2d(16, 16, kernel_size=3, stride=1, padding=1)\n#         self.bn = nn.BatchNorm2d(16)\n#         self.upsample = nn.Sequential(\n#             UpsampleBlock(16),\n#             nn.Conv2d(16, 3, kernel_size=9, stride=1, padding=4),\n#             nn.Tanh()\n#         )\n\n#     def forward(self, x):\n#         x1 = self.prelu(self.conv1(x))\n#         x2 = x1\n#         for block in self.res_blocks:\n#             x2 = checkpoint.checkpoint(block, x2)\n#         x3 = self.bn(self.conv2(x2)) + x1\n#         return self.upsample(x3)\n\n\n# class ResidualBlock(nn.Module):\n#     def __init__(self, channels):\n#         super(ResidualBlock, self).__init__()\n#         self.block = nn.Sequential(\n#             nn.Conv2d(channels, channels, kernel_size=3, stride=1, padding=1),\n#             nn.BatchNorm2d(channels),\n#             nn.PReLU(),\n#             nn.Conv2d(channels, channels, kernel_size=3, stride=1, padding=1),\n#             nn.BatchNorm2d(channels)\n#         )\n\n#     def forward(self, x):\n#         return x + self.block(x)\n\n\n# class UpsampleBlock(nn.Module):\n#     def __init__(self, in_channels):\n#         super(UpsampleBlock, self).__init__()\n#         self.block = nn.Sequential(\n#             nn.Conv2d(in_channels, in_channels * 4, kernel_size=3, stride=1, padding=1),\n#             nn.PixelShuffle(upscale_factor=2),\n#             nn.PReLU()\n#         )\n\n#     def forward(self, x):\n#         return self.block(x)\n\n\n# # Define Discriminator Network\n# class Discriminator(nn.Module):\n#     def __init__(self):\n#         super(Discriminator, self).__init__()\n#         self.model = nn.Sequential(\n#             nn.Conv2d(3, 16, kernel_size=3, stride=1, padding=1),  # Reduced initial channels\n#             nn.LeakyReLU(0.2, inplace=True),\n\n#             self._block(16, 32, stride=2),  # Reduced complexity\n#             self._block(32, 32, stride=2),\n\n#             nn.Flatten(),\n#             nn.Linear(32 * (640 // 4) * (1024 // 4), 128),  # Smaller dimensions\n#             nn.LeakyReLU(0.2, inplace=True),\n#             nn.Linear(128, 1)\n#         )\n\n#     def _block(self, in_channels, out_channels, stride):\n#         return nn.Sequential(\n#             nn.Conv2d(in_channels, out_channels, kernel_size=3, stride=stride, padding=1),\n#             nn.BatchNorm2d(out_channels),\n#             nn.LeakyReLU(0.2, inplace=True)\n#         )\n\n#     def forward(self, x):\n#         for layer in self.model:\n#             x = checkpoint.checkpoint(layer, x) if isinstance(layer, nn.Sequential) else layer(x)\n#         return x\n\n\n# # Additional memory-saving techniques can also be applied during training, such as mixed precision (using PyTorch's AMP).\n# # This reduced version should be much more memory-efficient and less computationally intensive.\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-15T19:55:07.423694Z","iopub.execute_input":"2024-12-15T19:55:07.424019Z","iopub.status.idle":"2024-12-15T19:55:07.436456Z","shell.execute_reply.started":"2024-12-15T19:55:07.423988Z","shell.execute_reply":"2024-12-15T19:55:07.435479Z"}},"outputs":[],"execution_count":7},{"cell_type":"code","source":"# Define Generator Network\nclass Generator(nn.Module):\n    def __init__(self):\n        super(Generator, self).__init__()\n        self.conv1 = nn.Conv2d(3, 16, kernel_size=9, stride=1, padding=4)  # Reduced channels\n        self.prelu = nn.PReLU()\n        self.res_blocks = nn.ModuleList([ResidualBlock(16) for _ in range(3)])  # Fewer residual blocks\n        self.conv2 = nn.Conv2d(16, 16, kernel_size=3, stride=1, padding=1)\n        self.bn = nn.BatchNorm2d(16)\n        self.upsample = nn.Sequential(\n            UpsampleBlock(16),\n            UpsampleBlock(16),\n            nn.Conv2d(16, 3, kernel_size=9, stride=1, padding=4),\n            nn.Tanh()\n        )\n\n    def forward(self, x):\n        x1 = self.prelu(self.conv1(x))\n        x2 = x1\n        for block in self.res_blocks:\n            x2 = checkpoint.checkpoint(block, x2)\n        x3 = self.bn(self.conv2(x2)) + x1\n        return self.upsample(x3)\n\n\nclass ResidualBlock(nn.Module):\n    def __init__(self, channels):\n        super(ResidualBlock, self).__init__()\n        self.block = nn.Sequential(\n            nn.Conv2d(channels, channels, kernel_size=3, stride=1, padding=1),\n            nn.BatchNorm2d(channels),\n            nn.PReLU(),\n            nn.Conv2d(channels, channels, kernel_size=3, stride=1, padding=1),\n            nn.BatchNorm2d(channels)\n        )\n\n    def forward(self, x):\n        return x + self.block(x)\n\n\nclass UpsampleBlock(nn.Module):\n    def __init__(self, in_channels):\n        super(UpsampleBlock, self).__init__()\n        self.block = nn.Sequential(\n            nn.Conv2d(in_channels, in_channels * 4, kernel_size=3, stride=1, padding=1),\n            nn.PixelShuffle(upscale_factor=2),\n            nn.PReLU()\n        )\n\n    def forward(self, x):\n        return self.block(x)\n\n\n# Define Discriminator Network\nclass Discriminator(nn.Module):\n    def __init__(self):\n        super(Discriminator, self).__init__()\n        self.model = nn.Sequential(\n            nn.Conv2d(3, 16, kernel_size=3, stride=1, padding=1),  # Reduced channels\n            nn.LeakyReLU(0.2, inplace=True),\n\n            self._block(16, 32, stride=2),\n            self._block(32, 32, stride=1),\n            self._block(32, 64, stride=2),\n            self._block(64, 64, stride=1),\n\n            nn.Flatten(),\n            nn.Linear(64 * (640 // 4) * (1024 // 4), 256),  # Reduced dimensions\n            nn.LeakyReLU(0.2, inplace=True),\n            nn.Linear(256, 1)\n        )\n\n    def _block(self, in_channels, out_channels, stride):\n        return nn.Sequential(\n            nn.Conv2d(in_channels, out_channels, kernel_size=3, stride=stride, padding=1),\n            nn.BatchNorm2d(out_channels),\n            nn.LeakyReLU(0.2, inplace=True)\n        )\n\n    def forward(self, x):\n        for layer in self.model:\n            x = checkpoint.checkpoint(layer, x) if isinstance(layer, nn.Sequential) else layer(x)\n        return x\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-15T19:55:07.438057Z","iopub.execute_input":"2024-12-15T19:55:07.438448Z","iopub.status.idle":"2024-12-15T19:55:07.456264Z","shell.execute_reply.started":"2024-12-15T19:55:07.438415Z","shell.execute_reply":"2024-12-15T19:55:07.455162Z"}},"outputs":[],"execution_count":8},{"cell_type":"markdown","source":"Getting out of memory error, again and again. How can i calculate how much momory the model will consume?","metadata":{}},{"cell_type":"code","source":"# # Define Generator Network\n# class Generator(nn.Module):\n#     def __init__(self):\n#         super(Generator, self).__init__()\n#         self.conv1 = nn.Conv2d(3, 32, kernel_size=9, stride=1, padding=4)\n#         self.prelu = nn.PReLU()\n#         self.res_blocks = nn.ModuleList([ResidualBlock(32) for _ in range(4)])  # ModuleList for checkpointing\n#         self.conv2 = nn.Conv2d(32, 32, kernel_size=3, stride=1, padding=1)\n#         self.bn = nn.BatchNorm2d(32)\n#         self.upsample = nn.Sequential(\n#             UpsampleBlock(32),\n#             UpsampleBlock(32),\n#             nn.Conv2d(32, 3, kernel_size=9, stride=1, padding=4),\n#             nn.Tanh()\n#         )\n\n#     def forward(self, x):\n#         x1 = self.prelu(self.conv1(x))\n#         x2 = x1\n#         for block in self.res_blocks:\n#             x2 = checkpoint.checkpoint(block, x2)  # Apply checkpointing here\n#         x3 = self.bn(self.conv2(x2)) + x1\n#         return self.upsample(x3)\n\n\n# class ResidualBlock(nn.Module):\n#     def __init__(self, channels):\n#         super(ResidualBlock, self).__init__()\n#         self.block = nn.Sequential(\n#             nn.Conv2d(channels, channels, kernel_size=3, stride=1, padding=1),\n#             nn.BatchNorm2d(channels),\n#             nn.PReLU(),\n#             nn.Conv2d(channels, channels, kernel_size=3, stride=1, padding=1),\n#             nn.BatchNorm2d(channels)\n#         )\n\n#     def forward(self, x):\n#         return x + self.block(x)\n\n\n# class UpsampleBlock(nn.Module):\n#     def __init__(self, in_channels):\n#         super(UpsampleBlock, self).__init__()\n#         self.block = nn.Sequential(\n#             nn.Conv2d(in_channels, in_channels * 4, kernel_size=3, stride=1, padding=1),\n#             nn.PixelShuffle(upscale_factor=2),\n#             nn.PReLU()\n#         )\n\n#     def forward(self, x):\n#         return self.block(x)\n\n\n# # Define Discriminator Network\n# class Discriminator(nn.Module):\n#     def __init__(self):\n#         super(Discriminator, self).__init__()\n#         self.model = nn.Sequential(\n#             nn.Conv2d(3, 32, kernel_size=3, stride=1, padding=1),\n#             nn.LeakyReLU(0.2, inplace=True),\n\n#             self._block(32, 32, stride=2),\n#             self._block(32, 64, stride=1),\n#             self._block(64, 64, stride=2),\n#             self._block(64, 128, stride=1),\n#             self._block(128, 128, stride=2),\n\n#             nn.Flatten(),\n#             nn.Linear(128 * (640 // 8) * (1024 // 8), 1024),  # Adjusted dimensions\n#             nn.LeakyReLU(0.2, inplace=True),\n#             nn.Linear(1024, 1)\n#         )\n\n#     def _block(self, in_channels, out_channels, stride):\n#         return nn.Sequential(\n#             nn.Conv2d(in_channels, out_channels, kernel_size=3, stride=stride, padding=1),\n#             nn.BatchNorm2d(out_channels),\n#             nn.LeakyReLU(0.2, inplace=True)\n#         )\n\n#     def forward(self, x):\n#         for layer in self.model:\n#             x = checkpoint.checkpoint(layer, x) if isinstance(layer, nn.Sequential) else layer(x)  # Apply checkpointing\n#         return x\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-15T19:55:07.457999Z","iopub.execute_input":"2024-12-15T19:55:07.458471Z","iopub.status.idle":"2024-12-15T19:55:07.474032Z","shell.execute_reply.started":"2024-12-15T19:55:07.458422Z","shell.execute_reply":"2024-12-15T19:55:07.472876Z"}},"outputs":[],"execution_count":9},{"cell_type":"code","source":"# # Define Generator Network\n# class Generator(nn.Module):\n#     def __init__(self):\n#         super(Generator, self).__init__()\n#         self.conv1 = nn.Conv2d(3, 32, kernel_size=9, stride=1, padding=4)\n#         self.prelu = nn.PReLU()\n#         self.res_blocks = nn.Sequential(*[ResidualBlock(32) for _ in range(4)])  # Reduced depth and channels\n#         self.conv2 = nn.Conv2d(32, 32, kernel_size=3, stride=1, padding=1)\n#         self.bn = nn.BatchNorm2d(32)\n#         self.upsample = nn.Sequential(\n#             UpsampleBlock(32),\n#             UpsampleBlock(32),\n#             nn.Conv2d(32, 3, kernel_size=9, stride=1, padding=4),\n#             nn.Tanh()\n#         )\n\n#     def forward(self, x):\n#         x1 = self.prelu(self.conv1(x))\n#         x2 = self.res_blocks(x1)\n#         x3 = self.bn(self.conv2(x2)) + x1\n#         return self.upsample(x3)\n\n\n# class ResidualBlock(nn.Module):\n#     def __init__(self, channels):\n#         super(ResidualBlock, self).__init__()\n#         self.block = nn.Sequential(\n#             nn.Conv2d(channels, channels, kernel_size=3, stride=1, padding=1),\n#             nn.BatchNorm2d(channels),\n#             nn.PReLU(),\n#             nn.Conv2d(channels, channels, kernel_size=3, stride=1, padding=1),\n#             nn.BatchNorm2d(channels)\n#         )\n\n#     def forward(self, x):\n#         return x + self.block(x)\n\n\n# class UpsampleBlock(nn.Module):\n#     def __init__(self, in_channels):\n#         super(UpsampleBlock, self).__init__()\n#         self.block = nn.Sequential(\n#             nn.Conv2d(in_channels, in_channels * 4, kernel_size=3, stride=1, padding=1),\n#             nn.PixelShuffle(upscale_factor=2),\n#             nn.PReLU()\n#         )\n\n#     def forward(self, x):\n#         return self.block(x)\n\n\n# # Define Discriminator Network\n# class Discriminator(nn.Module):\n#     def __init__(self):\n#         super(Discriminator, self).__init__()\n#         self.model = nn.Sequential(\n#             nn.Conv2d(3, 32, kernel_size=3, stride=1, padding=1),\n#             nn.LeakyReLU(0.2, inplace=True),\n            \n#             self._block(32, 32, stride=2),\n#             self._block(32, 64, stride=1),\n#             self._block(64, 64, stride=2),\n#             self._block(64, 128, stride=1),\n#             self._block(128, 128, stride=2),\n            \n#             nn.Flatten(),\n#             nn.Linear(128 * (640 // 8) * (1024 // 8), 1024),  # Adjusted dimensions\n#             nn.LeakyReLU(0.2, inplace=True),\n#             nn.Linear(1024, 1)\n#         )\n\n#     def _block(self, in_channels, out_channels, stride):\n#         return nn.Sequential(\n#             nn.Conv2d(in_channels, out_channels, kernel_size=3, stride=stride, padding=1),\n#             nn.BatchNorm2d(out_channels),\n#             nn.LeakyReLU(0.2, inplace=True)\n#         )\n\n#     def forward(self, x):\n#         return self.model(x)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-15T19:55:07.477183Z","iopub.execute_input":"2024-12-15T19:55:07.477561Z","iopub.status.idle":"2024-12-15T19:55:07.491171Z","shell.execute_reply.started":"2024-12-15T19:55:07.477527Z","shell.execute_reply":"2024-12-15T19:55:07.490028Z"}},"outputs":[],"execution_count":10},{"cell_type":"code","source":"# # Define Generator Network\n# class Generator(nn.Module):\n#     def __init__(self):\n#         super(Generator, self).__init__()\n#         self.conv1 = nn.Conv2d(3, 64, kernel_size=9, stride=1, padding=4)\n#         self.prelu = nn.PReLU()\n#         # self.res_blocks = nn.Sequential(*[ResidualBlock() for _ in range(16)])\n#         self.res_blocks = nn.Sequential(*[ResidualBlock() for _ in range(4)])\n#         self.conv2 = nn.Conv2d(64, 64, kernel_size=3, stride=1, padding=1)\n#         self.bn = nn.BatchNorm2d(64)\n#         self.upsample = nn.Sequential(\n#             UpsampleBlock(64),\n#             UpsampleBlock(64),\n#             nn.Conv2d(64, 3, kernel_size=9, stride=1, padding=4),\n#             nn.Tanh()\n#         )\n\n#     def forward(self, x):\n#         x1 = self.prelu(self.conv1(x))\n#         x2 = self.res_blocks(x1)\n#         x3 = self.bn(self.conv2(x2)) + x1\n#         return self.upsample(x3)\n\n# class ResidualBlock(nn.Module):\n#     def __init__(self):\n#         super(ResidualBlock, self).__init__()\n#         self.block = nn.Sequential(\n#             nn.Conv2d(64, 64, kernel_size=3, stride=1, padding=1),\n#             nn.BatchNorm2d(64),\n#             nn.PReLU(),\n#             nn.Conv2d(64, 64, kernel_size=3, stride=1, padding=1),\n#             nn.BatchNorm2d(64)\n#         )\n\n#     def forward(self, x):\n#         return x + self.block(x)\n\n# class UpsampleBlock(nn.Module):\n#     def __init__(self, in_channels):\n#         super(UpsampleBlock, self).__init__()\n#         self.block = nn.Sequential(\n#             nn.Conv2d(in_channels, in_channels * 4, kernel_size=3, stride=1, padding=1),\n#             nn.PixelShuffle(upscale_factor=2),\n#             nn.PReLU()\n#         )\n\n#     def forward(self, x):\n#         return self.block(x)\n\n# # Define Discriminator Network\n# class Discriminator(nn.Module):\n#     def __init__(self):\n#         super(Discriminator, self).__init__()\n#         self.model = nn.Sequential(\n#             nn.Conv2d(3, 64, kernel_size=3, stride=1, padding=1),\n#             nn.LeakyReLU(0.2, inplace=True),\n            \n#             self._block(64, 64, stride=2),\n#             self._block(64, 128, stride=1),\n#             self._block(128, 128, stride=2),\n#             self._block(128, 256, stride=1),\n#             self._block(256, 256, stride=2),\n#             self._block(256, 512, stride=1),\n#             self._block(512, 512, stride=2),\n\n#             nn.Flatten(),\n#             nn.Linear(512 * (640 // 16) * (1024 // 16), 1024),\n#             nn.LeakyReLU(0.2, inplace=True),\n#             nn.Linear(1024, 1)\n#         )\n\n#     def _block(self, in_channels, out_channels, stride):\n#         return nn.Sequential(\n#             nn.Conv2d(in_channels, out_channels, kernel_size=3, stride=stride, padding=1),\n#             nn.BatchNorm2d(out_channels),\n#             nn.LeakyReLU(0.2, inplace=True)\n#         )\n\n#     def forward(self, x):\n#         return self.model(x)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-15T19:55:07.492627Z","iopub.execute_input":"2024-12-15T19:55:07.493394Z","iopub.status.idle":"2024-12-15T19:55:07.506871Z","shell.execute_reply.started":"2024-12-15T19:55:07.493358Z","shell.execute_reply":"2024-12-15T19:55:07.505726Z"}},"outputs":[],"execution_count":11},{"cell_type":"code","source":"# Define Perceptual Loss\nclass PerceptualLoss(nn.Module):\n    def __init__(self):\n        super(PerceptualLoss, self).__init__()\n        vgg = vgg19(pretrained=True).features\n        self.features = nn.Sequential(*list(vgg)[:36]).eval()\n        for param in self.features.parameters():\n            param.requires_grad = False\n\n    def forward(self, sr, hr):\n        sr_features = self.features(sr)\n        hr_features = self.features(hr)\n        return nn.MSELoss()(sr_features, hr_features)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-15T19:55:07.508372Z","iopub.execute_input":"2024-12-15T19:55:07.508836Z","iopub.status.idle":"2024-12-15T19:55:07.525913Z","shell.execute_reply.started":"2024-12-15T19:55:07.508790Z","shell.execute_reply":"2024-12-15T19:55:07.524850Z"}},"outputs":[],"execution_count":12},{"cell_type":"code","source":"# # Define Perceptual Loss\n# class PerceptualLoss(nn.Module):\n#     def __init__(self):\n#         super(PerceptualLoss, self).__init__()\n#         vgg = vgg19(pretrained=True).features\n#         self.features = nn.Sequential(*list(vgg)[:36]).eval()\n#         for param in self.features.parameters():\n#             param.requires_grad = False\n\n#     def forward(self, sr, hr):\n#         sr_features = self.features(sr)\n#         hr_features = self.features(hr)\n#         return nn.MSELoss()(sr_features, hr_features)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-15T19:55:07.527394Z","iopub.execute_input":"2024-12-15T19:55:07.527761Z","iopub.status.idle":"2024-12-15T19:55:07.537774Z","shell.execute_reply.started":"2024-12-15T19:55:07.527728Z","shell.execute_reply":"2024-12-15T19:55:07.536710Z"}},"outputs":[],"execution_count":13},{"cell_type":"code","source":"# Save and Load Checkpoints\ndef save_checkpoint(generator, discriminator, gen_optimizer, disc_optimizer, epoch, filepath):\n    torch.save({\n        'generator_state_dict': generator.state_dict(),\n        'discriminator_state_dict': discriminator.state_dict(),\n        'gen_optimizer_state_dict': gen_optimizer.state_dict(),\n        'disc_optimizer_state_dict': disc_optimizer.state_dict(),\n        'epoch': epoch\n    }, filepath)\n\ndef load_checkpoint(filepath, generator, discriminator, gen_optimizer, disc_optimizer):\n    checkpoint = torch.load(filepath)\n    generator.load_state_dict(checkpoint['generator_state_dict'])\n    discriminator.load_state_dict(checkpoint['discriminator_state_dict'])\n    gen_optimizer.load_state_dict(checkpoint['gen_optimizer_state_dict'])\n    disc_optimizer.load_state_dict(checkpoint['disc_optimizer_state_dict'])\n    return checkpoint['epoch']\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-15T19:55:07.539047Z","iopub.execute_input":"2024-12-15T19:55:07.539416Z","iopub.status.idle":"2024-12-15T19:55:07.549348Z","shell.execute_reply.started":"2024-12-15T19:55:07.539382Z","shell.execute_reply":"2024-12-15T19:55:07.548309Z"}},"outputs":[],"execution_count":14},{"cell_type":"code","source":"# Training Loop\ndef train_srgan(generator, discriminator, train_loader, val_loader, gen_optimizer, disc_optimizer, perceptual_loss, adversarial_loss, device, epochs=10, checkpoint_path=\"srgan_checkpoint.pth\"):\n    start_epoch = 0\n    if checkpoint_path:\n        try:\n            start_epoch = load_checkpoint(checkpoint_path, generator, discriminator, gen_optimizer, disc_optimizer)\n            print(f\"Resumed training from epoch {start_epoch+1}\")\n        except FileNotFoundError:\n            print(\"No checkpoint found, starting training from scratch.\")\n\n    for epoch in range(start_epoch, epochs):\n        torch.cuda.empty_cache()\n\n        generator.train()\n        discriminator.train()\n        g_loss_epoch, d_loss_epoch = 0.0, 0.0\n\n        for noisy, gt in tqdm(train_loader):\n            noisy, gt = noisy.to(device), gt.to(device)\n\n            # Train Discriminator\n            disc_optimizer.zero_grad()\n            real_pred = discriminator(gt)\n            fake_images = generator(noisy)\n            fake_pred = discriminator(fake_images.detach())\n\n            real_loss = adversarial_loss(real_pred, torch.ones_like(real_pred, device=device))\n            fake_loss = adversarial_loss(fake_pred, torch.zeros_like(fake_pred, device=device))\n            d_loss = (real_loss + fake_loss) / 2\n            d_loss.backward()\n            disc_optimizer.step()\n\n            # Train Generator\n            gen_optimizer.zero_grad()\n            fake_pred = discriminator(fake_images)\n            g_loss = perceptual_loss(fake_images, gt) + 1e-3 * adversarial_loss(fake_pred, torch.ones_like(fake_pred, device=device))\n            g_loss.backward()\n            gen_optimizer.step()\n\n            g_loss_epoch += g_loss.item()\n            d_loss_epoch += d_loss.item()\n\n        print(f\"Epoch [{epoch+1}/{epochs}] Generator Loss: {g_loss_epoch:.4f}, Discriminator Loss: {d_loss_epoch:.4f}\")\n\n        # Save checkpoint after each epoch\n        save_checkpoint(generator, discriminator, gen_optimizer, disc_optimizer, epoch, checkpoint_path)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-15T19:55:07.550718Z","iopub.execute_input":"2024-12-15T19:55:07.551044Z","iopub.status.idle":"2024-12-15T19:55:07.564720Z","shell.execute_reply.started":"2024-12-15T19:55:07.551014Z","shell.execute_reply":"2024-12-15T19:55:07.563793Z"}},"outputs":[],"execution_count":15},{"cell_type":"code","source":"# # Training Loop\n# def train_srgan(generator, discriminator, train_loader, val_loader, gen_optimizer, disc_optimizer, perceptual_loss, adversarial_loss, device, epochs=10):\n#     for epoch in range(epochs):\n#         torch.cuda.empty_cache()\n\n#         generator.train()\n#         discriminator.train()\n#         g_loss_epoch, d_loss_epoch = 0.0, 0.0\n\n#         for noisy, gt in tqdm(train_loader):\n#             noisy, gt = noisy.to(device), gt.to(device)\n\n#             # Train Discriminator\n#             disc_optimizer.zero_grad()\n#             real_pred = discriminator(gt)\n#             fake_images = generator(noisy)\n#             fake_pred = discriminator(fake_images.detach())\n\n#             real_loss = adversarial_loss(real_pred, torch.ones_like(real_pred, device=device))\n#             fake_loss = adversarial_loss(fake_pred, torch.zeros_like(fake_pred, device=device))\n#             d_loss = (real_loss + fake_loss) / 2\n#             d_loss.backward()\n#             disc_optimizer.step()\n\n#             # Train Generator\n#             gen_optimizer.zero_grad()\n#             fake_pred = discriminator(fake_images)\n#             g_loss = perceptual_loss(fake_images, gt) + 1e-3 * adversarial_loss(fake_pred, torch.ones_like(fake_pred, device=device))\n#             g_loss.backward()\n#             gen_optimizer.step()\n\n#             g_loss_epoch += g_loss.item()\n#             d_loss_epoch += d_loss.item()\n\n#         print(f\"Epoch [{epoch+1}/{epochs}] Generator Loss: {g_loss_epoch:.4f}, Discriminator Loss: {d_loss_epoch:.4f}\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-15T19:55:07.565894Z","iopub.execute_input":"2024-12-15T19:55:07.566220Z","iopub.status.idle":"2024-12-15T19:55:07.580531Z","shell.execute_reply.started":"2024-12-15T19:55:07.566190Z","shell.execute_reply":"2024-12-15T19:55:07.579326Z"}},"outputs":[],"execution_count":16},{"cell_type":"code","source":"# Initialize Models and Optimizers\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\ngenerator = Generator().to(device)\ndiscriminator = Discriminator().to(device)\n\nperceptual_loss = PerceptualLoss().to(device)\nadversarial_loss = nn.BCEWithLogitsLoss().to(device)\n\ngen_optimizer = optim.Adam(generator.parameters(), lr=1e-4)\ndisc_optimizer = optim.Adam(discriminator.parameters(), lr=1e-4)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-15T19:55:07.582165Z","iopub.execute_input":"2024-12-15T19:55:07.582542Z","iopub.status.idle":"2024-12-15T19:55:19.445947Z","shell.execute_reply.started":"2024-12-15T19:55:07.582504Z","shell.execute_reply":"2024-12-15T19:55:19.444856Z"}},"outputs":[{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n  warnings.warn(\n/opt/conda/lib/python3.10/site-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=VGG19_Weights.IMAGENET1K_V1`. You can also use `weights=VGG19_Weights.DEFAULT` to get the most up-to-date weights.\n  warnings.warn(msg)\nDownloading: \"https://download.pytorch.org/models/vgg19-dcbb9e9d.pth\" to /root/.cache/torch/hub/checkpoints/vgg19-dcbb9e9d.pth\n100%|██████████| 548M/548M [00:03<00:00, 183MB/s] \n","output_type":"stream"}],"execution_count":17},{"cell_type":"code","source":"# # Initialize Models and Optimizers\n# device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n# generator = Generator().to(device)\n# discriminator = Discriminator().to(device)\n\n# perceptual_loss = PerceptualLoss().to(device)\n# adversarial_loss = nn.BCEWithLogitsLoss().to(device)\n\n# gen_optimizer = optim.Adam(generator.parameters(), lr=1e-4)\n# disc_optimizer = optim.Adam(discriminator.parameters(), lr=1e-4)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-15T19:55:19.447720Z","iopub.execute_input":"2024-12-15T19:55:19.448450Z","iopub.status.idle":"2024-12-15T19:55:19.453270Z","shell.execute_reply.started":"2024-12-15T19:55:19.448399Z","shell.execute_reply":"2024-12-15T19:55:19.452120Z"}},"outputs":[],"execution_count":18},{"cell_type":"code","source":"# # Training Setup\n# device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n\n# # Instantiate the model\n# model = ResNetUNet(pretrained=True).to(device)\n\n# # Define the loss function and optimizer\n# criterion = nn.MSELoss()  # Or SSIMLoss for perceptual quality\n# optimizer = optim.Adam(model.parameters(), lr=1e-4)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-15T19:55:19.455089Z","iopub.execute_input":"2024-12-15T19:55:19.455504Z","iopub.status.idle":"2024-12-15T19:55:19.610624Z","shell.execute_reply.started":"2024-12-15T19:55:19.455457Z","shell.execute_reply":"2024-12-15T19:55:19.609561Z"}},"outputs":[],"execution_count":19},{"cell_type":"code","source":"# # Initialize Models and Optimizers for Multi-GPU Training\n# device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n\n# # Wrap the models with DataParallel\n# generator = nn.DataParallel(Generator()).to(device)\n# discriminator = nn.DataParallel(Discriminator()).to(device)\n\n# perceptual_loss = PerceptualLoss().to(device)\n# adversarial_loss = nn.BCEWithLogitsLoss().to(device)\n\n# gen_optimizer = optim.Adam(generator.parameters(), lr=1e-4)\n# disc_optimizer = optim.Adam(discriminator.parameters(), lr=1e-4)\n\n# # Train the Model on Multi-GPUs\n# train_srgan(generator, discriminator, train_loader, val_loader, gen_optimizer, disc_optimizer,\n#             perceptual_loss, adversarial_loss, device, epochs=8, checkpoint_path=\"srgan_checkpoint.pth\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-15T19:55:19.611880Z","iopub.execute_input":"2024-12-15T19:55:19.612219Z","iopub.status.idle":"2024-12-15T19:55:19.621399Z","shell.execute_reply.started":"2024-12-15T19:55:19.612187Z","shell.execute_reply":"2024-12-15T19:55:19.620417Z"}},"outputs":[],"execution_count":20},{"cell_type":"code","source":"# Train the Model\ntrain_srgan(generator, discriminator, train_loader, val_loader, gen_optimizer, disc_optimizer, perceptual_loss, adversarial_loss, device, epochs=8, checkpoint_path=\"srgan_checkpoint.pth\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-15T19:55:19.622722Z","iopub.execute_input":"2024-12-15T19:55:19.623129Z"}},"outputs":[{"name":"stderr","text":"/tmp/ipykernel_24/1997784574.py:12: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n  checkpoint = torch.load(filepath)\n","output_type":"stream"},{"name":"stdout","text":"No checkpoint found, starting training from scratch.\n","output_type":"stream"},{"name":"stderr","text":"  0%|          | 0/139 [00:00<?, ?it/s]/opt/conda/lib/python3.10/site-packages/torch/_dynamo/eval_frame.py:600: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.4 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n  return fn(*args, **kwargs)\n/opt/conda/lib/python3.10/site-packages/torch/utils/checkpoint.py:295: FutureWarning: `torch.cpu.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cpu', args...)` instead.\n  with torch.enable_grad(), device_autocast_ctx, torch.cpu.amp.autocast(**ctx.cpu_autocast_kwargs):  # type: ignore[attr-defined]\n100%|██████████| 139/139 [7:21:33<00:00, 190.60s/it]  \n","output_type":"stream"},{"name":"stdout","text":"Epoch [1/8] Generator Loss: 39.0977, Discriminator Loss: 94.3174\n","output_type":"stream"},{"name":"stderr","text":"  0%|          | 0/139 [00:00<?, ?it/s]","output_type":"stream"}],"execution_count":null},{"cell_type":"code","source":"# # Train the Model\n# train_srgan(generator, discriminator, train_loader, val_loader, gen_optimizer, disc_optimizer, perceptual_loss, adversarial_loss, device, epochs=8)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-15T06:24:05.137996Z","iopub.status.idle":"2024-12-15T06:24:05.138449Z","shell.execute_reply.started":"2024-12-15T06:24:05.138210Z","shell.execute_reply":"2024-12-15T06:24:05.138232Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"class LowLightDataset(Dataset):\n    def __init__(self, noisy_dir, gt_dir=None, transform_noisy=None, transform_gt=None):\n        self.noisy_dir = noisy_dir\n        self.gt_dir = gt_dir\n        self.noisy_files = sorted(os.listdir(noisy_dir))\n        self.gt_files = sorted(os.listdir(gt_dir)) if gt_dir else None\n        self.transform_noisy = transform_noisy\n        self.transform_gt = transform_gt\n\n    def __len__(self):\n        return len(self.noisy_files)\n\n    def __getitem__(self, idx):\n        # Load noisy image\n        noisy_path = os.path.join(self.noisy_dir, self.noisy_files[idx])\n        noisy_image = Image.open(noisy_path).convert('RGB')\n        if self.transform_noisy:\n            noisy_image = self.transform_noisy(noisy_image)\n\n        # Include filename\n        filename = os.path.basename(noisy_path)\n\n        # Load ground truth image if available\n        if self.gt_dir:\n            gt_path = os.path.join(self.gt_dir, self.gt_files[idx])\n            gt_image = Image.open(gt_path).convert('RGB')\n            if self.transform_gt:\n                gt_image = self.transform_gt(gt_image)\n            return noisy_image, gt_image, filename\n\n        return noisy_image, filename\n\ntest_dataset = LowLightDataset(test_noisy_dir, transform_noisy=transform_noisy)\ntest_loader = DataLoader(test_dataset, batch_size=16, shuffle=False, num_workers=4)\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"from tqdm import tqdm\nfrom PIL import Image\nimport matplotlib.pyplot as plt\n# i = 0\n# Generate Submission\ndef save_test_predictions(model, dataloader, output_dir, device):\n    i = 0\n    model.eval()\n    os.makedirs(output_dir, exist_ok=True)\n    with torch.no_grad():\n        for noisy_img, filenames in tqdm(test_loader):\n            noisy_img = noisy_img.to(device)\n            outputs = model(noisy_img).cpu()\n            # i += 1\n            # if i<=3:\n            #     print(outputs.shape)\n            outputs = outputs * 0.5 + 0.5  # Denormalize to [0, 1]\n            outputs = outputs.clamp(0, 1)\n            for i, filename in enumerate(filenames):\n                # submission.append(img.permute(1, 2, 0).numpy())\n                output_path = os.path.join(output_dir, f\"{filename.split('.')[0]}.png\")\n                curr_img = outputs[i].permute(1, 2, 0).numpy()\n                # plt.imsave(output_path, curr_img, cmap='viridis')\n                curr_img = (curr_img * 255).astype('uint8')     # Convert [0, 1] to [0, 255]\n                Image.fromarray(curr_img).save(output_path)     # Save as an RGB image\n                # plt.imsave(output_path, curr_img)\n\n# Directory to save test predictions\ntest_output_dir = \"test_outputs_for_pred\"\nsave_test_predictions(model, test_loader, test_output_dir, device)\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import os\nimport numpy as np\nimport pandas as pd\nfrom PIL import Image\n\ndef images_to_csv(folder_path, output_csv):\n    data_rows = []\n    for filename in os.listdir(folder_path):\n        if filename.endswith(('.png', '.jpg', '.jpeg', '.bmp', '.tiff')):\n            image_path = os.path.join(folder_path, filename)\n            image = Image.open(image_path).convert('L') \n            image_array = np.array(image).flatten()[::8]\n            data_rows.append([filename.split('.')[0], *image_array])\n    column_names = ['ID'] + [f'pixel_{i}' for i in range(len(data_rows[0]) - 1)]\n    df = pd.DataFrame(data_rows, columns=column_names)\n    df.to_csv(output_csv, index=False)\n    print(f'Successfully saved to {output_csv}')\n\nfolder_path = '/kaggle/working/test_outputs_for_pred'\noutput_csv = 'submission.csv'\nimages_to_csv(folder_path, output_csv)\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"out_csv = pd.read_csv('submission.csv')\nout_csv","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}